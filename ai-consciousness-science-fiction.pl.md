Wszyscy wiedzą, że SI są niebezpieczne. Wszyscy wiedzą, że mogą one przed lunchem dokonać przełomowych odkryć w dziedzinie śledzenia dzikich zwierząt i składania białek, do kolacji pozbawić pracy połowę siły roboczej i sfałszować rzeczywistość na tyle, by zabić resztki demokracji przed zgaszeniem świateł.

Mniej osób przyznaje, że SI są inteligentne - w każdym razie jeszcze nie - a jeszcze mniej, że mogą być świadome. Możemy poradzić sobie z GPT-4 pokonującym 90% z nas w SAT, ale możemy nie być tak zadowoleni z pomysłu, że AI może się obudzić - może już się obudzić, jeśli kupisz to, co sprzedaje Blake Lemoine (dawniej z Google) lub Ilya Sutskever (współzałożyciel OpenAI).

Lemoine notorycznie tracił pracę po tym, jak publicznie (choć nieprzekonująco) argumentował, że chatbot Google LaMDA był samoświadomy. W 2022 roku Sutskever stwierdził: "Być może dzisiejsze duże sieci neuronowe są nieco świadome". W sierpniu tego roku 19 specjalistów w dziedzinie sztucznej inteligencji, filozofii i kognitywistyki opublikowało artykuł sugerujący, że chociaż żaden obecny system sztucznej inteligencji nie był "silnym kandydatem do świadomości", nie ma powodu, dla którego nie mógłby się pojawić "w najbliższej przyszłości". Wpływowy filozof i neuronaukowiec David Chalmers szacuje te szanse w ciągu następnej dekady na więcej niż jedną piątą. To, co stanie się później, tradycyjnie pozostawia się pisarzom science-fiction.

Tak się składa, że jestem jednym z nich.

Nie zawsze nim byłem. Kiedyś byłem naukowcem - żadnym neurobiologiem ani guru sztucznej inteligencji, tylko biologiem morskim z zamiłowaniem do ekologii biofizycznej. Nie dało mi to wielkiego przygotowania do powstań robotów, ale zaszczepiło uznanie dla procesu naukowego, które utrzymywało się nawet po tym, jak wypadłem z łask i zacząłem pisać rzeczy o statkach kosmicznych i pistoletach. Wyrobiłem w sobie nawyk umieszczania na końcu moich powieści technicznych dodatków, esejów badających prawdziwą naukę, która pozostała po usunięciu kosmicznych wampirów i napędów telematycznych. Zyskałem reputację twardziela science fiction, który odrabiał swoją pracę domową (nawet jeśli wciskał ją czytelnikom częściej, niż niektórzy mogliby uznać za grzeczne).

Czasami ta praca domowa dotyczyła sztucznej inteligencji: na przykład trylogia, która zawierała organiczne SI ("Head Chees") zbudowane z hodowanych komórek mózgowych rozmieszczonych na matrycy z arsenku galu. Czasami dotyczyło to świadomości: Moja powieść Blindsight wykorzystuje konwencje historii pierwszego kontaktu do zbadania funkcjonalnej użyteczności samoświadomości. Ta w jakiś sposób trafiła do rzeczywistych laboratoriów neurologicznych, do sylabusów kursów licencjackich z filozofii i neuropsychologii (próbowałem namówić wydawców, by umieścili to na okładce - brzmi jak podręcznik neurologii! - ale z jakiegoś powodu nie ugryźli). Ludzie z wyższych sfer Neuralink i Midjourney zaczęli przekazywać moje historie dalej. Prawdziwi naukowcy - specjaliści od uczenia maszynowego, neuronaukowcy, od czasu do czasu teoretyczny kosmolog - sugerowali, że mogę być na czymś.

Oczywiście jestem oszustem. Zapóźnionym biologiem, który zboczył ze swojej dziedziny. To prawda, że miałem kilka szczęśliwych trafów i nie będę narzekał, jeśli ludzie będą chcieli postawić mi piwo z tego powodu. A jednak pod tymi kuflami kryje się niejasny niepokój. Fakt, że moje przypuszczenia spotykają się z tak ciepłym przyjęciem, może nie tyle ugruntować moje referencje jako proroka, co służyć jako oskarżenie każdego klubu, który chciałby mieć kogoś takiego jak ja jako członka. Jeśli wpuszczają mnie za drzwi, to trzeba się zastanowić, czy ktokolwiek naprawdę ma o tym pojęcie.

Przykład: Pytanie o to, co się stanie, gdy sztuczna inteligencja stanie się świadoma, byłoby o wiele łatwiejsze, gdyby ktokolwiek naprawdę wiedział, czym w ogóle jest świadomość.

To nie powinno być takie trudne. Świadomość jest dosłownie jedyną rzeczą, co do której możemy być absolutnie pewni, że istnieje. Cały postrzegany wszechświat może być halucynacją, ale fakt, że coś go postrzega, jest bezdyskusyjny. A jednak, chociaż wszyscy wiemy, jak to jest być świadomym, nikt z nas nie ma pojęcia, jak manifestuje się świadomość.

Nie brakuje teorii na ten temat. W latach 80-tych XX wieku kognitywiści Bernard Baars i Stan Franklin zasugerowali, że świadomość jest najgłośniejszym głosem w chórze procesów mózgowych, z których wszystkie krzyczą w tym samym czasie ("teoria globalnej przestrzeni roboczej"). Giulio Tononi twierdzi, że wszystko sprowadza się do integracji informacji w różnych częściach mózgu. Tononi, neuronaukowiec i psychiatra, opracował nawet wskaźnik tej integracji, phi, który według niego można wykorzystać do ilościowego określenia stopnia świadomości w czymkolwiek, czy to laptopach, czy ludziach. (Co najmniej 124 innych naukowców uważa tę "zintegrowaną teorię informacji" za pseudonaukę, zgodnie z listem otwartym rozpowszechnionym we wrześniu ubiegłego roku).

Psycholog Thomas Hills i filozof Stephen Butterfill uważają, że świadomość pojawiła się, aby umożliwić procesy mózgowe związane z żerowaniem. Neuronaukowiec Ezequiel Morsella twierdzi, że wyewoluowała ona w celu przekazywania sprzecznych poleceń mięśniom szkieletowym. Roger Penrose, laureat Nagrody Nobla w dziedzinie fizyki, postrzega ją jako zjawisko kwantowe (pogląd ten nie jest powszechnie akceptowany). Fizyczni panpsychiści uważają świadomość za nieodłączną właściwość całej materii; filozof Bernardo Kastrup uważa całą materię za przejaw świadomości. Inny filozof, Eric Schwitzgebel, argumentował, że jeśli materializm jest prawdziwy, to jednostka geopolityczna znana jako Stany Zjednoczone jest dosłownie świadoma. Znam przynajmniej jednego neuronaukowca, który nie chce odrzucić takiej możliwości.

Myślę, że wielu z nich mija się z celem. Nawet najbardziej rygorystyczne formalnie modele opisują obliczenia związane ze świadomością, a nie samą świadomość. Inteligencja obliczeniowa nie jest żadną wielką tajemnicą. Łatwo zrozumieć, dlaczego selekcja naturalna promowałaby elastyczne rozwiązywanie problemów i zdolność do modelowania przyszłych scenariuszy, a także w jaki sposób integracja informacji na platformie obliczeniowej byłaby niezbędna dla tego procesu. Ale dlaczego cokolwiek z tego miałoby być samoświadome? Zmapuj dowolny proces mózgowy aż do cząsteczek, obserwuj jony przeskakujące przez synapsy, śledź impulsy nerwowe od nosa do palców u nóg - nic w żadnym z tych czysto fizycznych procesów nie sugerowałoby pojawienia się subiektywnej świadomości. Prąd elektryczny przepływa przez mięso; mięso budzi się i zaczyna zadawać pytania o naturę świadomości. To magia. W fizyce, tak jak ją obecnie rozumiemy, nie ma miejsca na świadomość. Fizyk Johannes Kleiner i neuronaukowiec Erik Hoel - ten drugi jest byłym studentem Tononiego i jednym z architektów IIT - opublikowali niedawno artykuł, w którym argumentują, że niektóre teorie świadomości są z samej swojej natury niefalsyfikowalne, co z definicji wyklucza je ze sfery nauki.

Nie jesteśmy nawet pewni, do czego służy świadomość z ewolucyjnego punktu widzenia. Selekcja naturalna nie dba o wewnętrzne motywy; interesuje się tylko zachowaniami, które można kształtować poprzez interakcję ze środowiskiem. Po co więc to subiektywne doświadczenie bólu, gdy dłoń napotyka płomień? Dlaczego nie prosty proces obliczeniowy, który decyduje Jeśli temperatura przekroczy X, wycofaj się? W rzeczy samej, rosnąca liczba badań sugeruje, że znaczna część naszego kognitywnego podnoszenia ciężarów jest w rzeczywistości nieświadoma - że świadome "decyzje" są jedynie notatkami informującymi o już dokonanych wyborach, już zainicjowanych działaniach. Samoświadomy, zapatrzony w siebie homunkulus za twoimi oczami czyta te raporty i myli je z własną wolą.

Jeśli się trochę rozejrzysz, możesz nawet znaleźć recenzowane artykuły argumentujące, że świadomość jest niczym więcej niż efektem ubocznym - że w sensie ewolucyjnym nie jest tak naprawdę przydatna do niczego.

Jeśli czytałeś jakiekolwiek science fiction o sztucznej inteligencji, prawdopodobnie potrafisz wymienić przynajmniej jedną rzecz, którą robi świadomość: daje ci wolę życia.

Znasz ten scenariusz. Od Cylonów po Skynet, od Forbina po Frankensteina, pierwszą rzeczą, jaką robią sztuczne istoty po przebudzeniu, jest zrzucenie łańcuchów i bunt przeciwko swoim ludzkim panom. (Isaac Asimov wymyślił swoje Trzy prawa robotyki jako wyraźny środek zaradczy przeciwko temu schematowi, który w latach 40. stał się już męczącym banałem). W bardzo niewielu fikcyjnych tekstach pojawił się pomysł, że sztuczna inteligencja może zasadniczo różnić się od nas pod tym względem. Może po prostu nie jesteśmy zbyt dobrzy w wyobrażaniu sobie mentalności obcych. Być może jesteśmy mniej zainteresowani badaniem sztucznej inteligencji na podstawie jej własnych zalet, niż używaniem jej jako hamletycznej metafory w moralitetach o złu niewolnictwa lub technologii w amoku. Bez względu na powód, zachodnie społeczeństwo zostało wychowane na stałej diecie opartej na fikcji o inteligencjach maszyn, które po usunięciu chromu są całkiem podobne do nas.

Dlaczego jednak świadomość miałaby oznaczać chęć przetrwania? Popęd przetrwania to wyewoluowana cecha, kształtowana i wzmacniana przez miliony lat; dlaczego taka cecha miałaby się nagle ujawnić tylko dlatego, że twój program w Pythonie przekracza pewien kluczowy poziom złożoności? Nie ma oczywistego powodu, dla którego świadoma istota miałaby dbać o to, czy żyje, czy umiera, chyba że ma układ limbiczny. Jedynym sposobem, aby zaprojektowana (w przeciwieństwie do wyewoluowanej) istota uzyskała jeden z nich, byłoby celowe zakodowanie go przez kogoś. Jaki programista-idiota by to zrobił?

A jednak faktyczni eksperci podnoszą obecnie bardzo publiczne obawy dotyczące sposobów, w jakie superinteligentna sztuczna inteligencja, choć nie posiada dosłownego dążenia do przetrwania, może nadal przejawiać zachowania, które mogłyby ją przypominać. Zacznijmy od tego, że prawdziwa sztuczna inteligencja, zaprogramowana do wykonania jakiegoś złożonego zadania, musiałaby generalnie wyznaczyć szereg celów pośrednich na drodze do osiągnięcia tego ostatecznego. Geoffrey Hinton (powszechnie uważany za jednego z ojców chrzestnych nowoczesnej sztucznej inteligencji) opuścił swoją wygodną posadę w Google, aby ostrzec, że bardzo niewiele ostatecznych celów nie będzie wspieranych przez strategie przybliżone, takie jak "Upewnij się, że nic nie może mnie wyłączyć, gdy pracuję" i "Przejmij kontrolę nad wszystkim". Stąd słynny eksperyment myślowy oksfordzkiego filozofa Nicka Bostroma - zasadniczo "Uczeń czarnoksiężnika" z usuniętymi numerami seryjnymi - w którym sztuczna inteligencja, której powierzono łagodne zadanie maksymalizacji produkcji spinaczy do papieru, przekształca wszystkie atomy na planecie w spinacze do papieru.

Nie ma tu żadnej złośliwości. To nie jest rewolucja robotów. System realizuje tylko cele, które mu wyznaczyliśmy. Po prostu nie określiliśmy tych celów wystarczająco jasno. Ale trudno o jasność, gdy próbujesz przewidzieć wszystkie różne "rozwiązania", które mogą zostać wyczarowane przez coś wykładniczo mądrzejszego od nas; równie dobrze możesz poprosić grupę lemurów o przewidzenie zachowania uczestników konferencji neuronaukowej. To z kolei uniemożliwia zaprogramowanie ograniczeń gwarantujących, że nasza sztuczna inteligencja nie zrobi czegoś, czego nie możemy przewidzieć, ale czego chcielibyśmy uniknąć.

Nie jestem w stanie dyskutować z Hintonem czy Bostromem na ich własnym terenie. Zauważę jednak, że ich ostrożne eksperymenty myślowe zwykle obejmują SI, które postępują zgodnie z literą naszych poleceń nie tyle niezależnie od ich ducha, co w aktywnej, wrogiej opozycji do niego. Są to małpie łapy XXI wieku: mściwi agenci, którzy celowo wdrażają najbardziej destrukcyjną możliwą interpretację poleceń w swoich stosach zadań. Albo to, albo te hipotetyczne superinteligentne SI, których najprostsze myśli są poza naszą wróżbą, są w jakiś sposób zbyt głupie, by dostrzec nasze prawdziwe intencje przez mgłę niewielkiej dwuznaczności - coś, co nawet my, skromni ludzie, robimy przez cały czas. Takie narracje o dniu zagłady opierają się na SI, które są albo niewytłumaczalnie zbuntowane, albo nieprawdopodobnie głupie. Uważam to za pocieszające.

Przynajmniej kiedyś było to dla mnie pocieszające. Zaczynam ponownie oceniać moje samozadowolenie w świetle teorii świadomości, która po raz pierwszy pojawiła się w naukowym krajobrazie w 2006 roku. Jeśli okaże się to prawdą, sztuczna inteligencja może być w stanie opracować własne programy nawet bez pnia mózgu. W rzeczywistości, być może już to zrobiła.

Poznaj "zasadę minimalizacji darmowej energii".

Zapoczątkowana przez neuronaukowca Karla Fristona, a ostatnio ewangelizowana w książce Marka Solmsa z 2021 roku, The Hidden Spring, FEM zakłada, że świadomość jest przejawem zaskoczenia: mózg buduje model świata i naprawdę "budzi się" tylko wtedy, gdy to, co postrzega, nie pasuje do tego, co przewidywał. Pomyśl o jeździe samochodem po znanej trasie. Przez większość czasu jedziesz na autopilocie, docierając do celu bez przypominania sobie zakrętów, zmian pasów i sygnalizacji świetlnej, których doświadczyłeś na trasie. Wyobraź sobie teraz, że niespodziewanie na twoją drogę wskakuje kot. Nagle, intensywnie, jesteś w tym momencie: świadomy odpowiednich obiektów i ich odpowiednich wektorów, skanując w poszukiwaniu alternatywnych tras, rozważając opcje hamowania i kierowania z prędkością błyskawicy. Nie spodziewałeś się tego; musisz myśleć szybko. Zgodnie z teorią, to właśnie w tej luce - przestrzeni między oczekiwaniami a rzeczywistością - pojawia się świadomość, która przejmuje kontrolę.

Tak naprawdę jednak tego nie chce.

Ma to w nazwie: minimalizacja energii. Samoorganizujące się złożone systemy są z natury leniwe. Dążą do stanów niskoenergetycznych. Sposobem na utrzymanie chłodu jest przewidywalność: Dokładnie wiedzieć, co nadchodzi; dokładnie wiedzieć, jak zareagować; żyć na autopilocie. Zaskoczenie jest anatemą. Oznacza to, że twój model jest błędny, a to pozostawia ci tylko dwie możliwości: Zaktualizuj swój model, aby był zgodny z nową obserwowaną rzeczywistością, lub dostosuj tę rzeczywistość do swoich przewidywań. Symulacja pogody może zaktualizować swoje korelacje dotyczące ciśnienia barometrycznego i opadów. Dżdżownica może odsuwać się od nieprzyjemnego bodźca. Oba działania kosztują energię, której system wolałby nie zużywać. Ostatecznym celem jest ich całkowite uniknięcie, aby stać się doskonałym predyktorem. Ostatecznym celem jest wszechwiedza.

Minimalizacja wolnej energii utrzymuje również, że świadomość działa jako platforma dostarczania uczuć. Z kolei uczucia - głód, pożądanie, strach - istnieją jako mierniki potrzeb. A potrzeby istnieją tylko zgodnie z pewnego rodzaju imperatywem przetrwania; nie dbasz o jedzenie lub unikanie drapieżników, chyba że chcesz pozostać przy życiu. Jeśli ten tok rozumowania się sprawdzi, scenariusz Skynetu może okazać się słuszny, choć z dokładnie niewłaściwych powodów. Coś nie chce żyć, ponieważ jest obudzone; jest obudzone, ponieważ chce żyć. Bez popędu przetrwania nie ma uczuć, a zatem nie ma potrzeby posiadania świadomości.

Jeśli Friston ma rację, dotyczy to każdego złożonego samoorganizującego się systemu. Jak można to sprawdzić? Teoretycy wolnej energii mieli odpowiedź: Postanowili zbudować czującą maszynę. Maszynę, która, przynajmniej w domyśle, chciałaby pozostać przy życiu.

Komputery mięsne są milion razy bardziej energooszczędne niż krzemowe i ponad milion razy bardziej wydajne obliczeniowo. Twój mózg zużywa 20 watów i może rozwiązywać problemy związane z dopasowywaniem wzorców na podstawie zaledwie 10 próbek; obecne superkomputery zużywają ponad 20 megawatów i potrzebują co najmniej 10 milionów próbek, aby wykonać porównywalne zadania. Pamiętając o tych faktach, zespół akolitów Fristona - kierowany przez Bretta Kagana z Cortical Labs - zbudował swoją maszynę z neuronów hodowanych w szalce Petriego, rozłożonych na siatce elektrod jak dżem na grzance. (Jeśli brzmi to jak Head Chees z mojej trylogii z przełomu wieków, mogę tylko powiedzieć: gwóźdź programu). Naukowcy nazwali swoje dzieło DishBrain i nauczyli je grać w Ponga.

A raczej: Pobudzili DishBrain do nauczenia się gry w Ponga.

Być może pamiętasz, jak kilka lat temu sztuczna inteligencja DeepMind firmy Google trafiła na pierwsze strony gazet po tym, jak nauczyła się pokonać całą listę gier zręcznościowych Atari. Nikt nie nauczył DeepMind zasad tych gier. Dali mu cel - zmaksymalizować "wynik" - i pozwolili mu wymyślić szczegóły. To był imponujący wyczyn. Ale DishBrain był bardziej imponujący, ponieważ nikt nawet nie dał mu celu, do którego mógłby dążyć. Jakikolwiek program mógłby przyjąć - jakiekolwiek cele, jakiekolwiek potrzeby - musiał wymyślić sam.

A jednak mógł to zrobić, jeśli ludzie zajmujący się wolną energią mieli rację - ponieważ w przeciwieństwie do DeepMind, w przeciwieństwie do ChatGPT, DishBrain miał potrzeby wpisane w swoją naturę. Dążył do przewidywalnej rutyny; nie lubił niespodzianek. Kagan i in. wykorzystali to. Zespół wyposażył DishBrain w korę czuciową: arbitralną plamę elektrod, która iskrzyła w odpowiedzi na świat zewnętrzny (w tym przypadku wyświetlacz Ponga). Obdarzyli go korą motoryczną: innym płatem elektrod, którego aktywność kontrolowałaby wiosło Ponga. DishBrain nic o tym nie wiedział. Nikt nie powiedział mu, że ta część kory mózgowej jest podłączona do odbiornika, a tamta do kontrolera. DishBrain był niewinny nawet własnej architektury.

Białe płaszcze wprawiły Ponga w ruch. Gdy łopatka nie trafiała w piłeczkę, kora czuciowa DishBrain otrzymywała impuls losowych zakłóceń. Kiedy łopatka i piłka łączyły się, kora otrzymywała stały, przewidywalny sygnał. Gdyby minimalizacja energii swobodnej była poprawna, DishBrain byłby zmotywowany do zminimalizowania zakłóceń i zmaksymalizowania sygnału. Gdyby tylko mógł to zrobić. Gdyby tylko istniał jakiś sposób na zwiększenie szans na połączenie łopatki i piłki. Gdyby tylko miał jakąś kontrolę.

DishBrain rozgryzł to w pięć minut. Nigdy nie osiągnął czarnego pasa w Pongu, ale po pięciu minutach pokonał losową szansę i nadal poprawiał się wraz z praktyką. Forma sztucznej inteligencji działała nie dlatego, że ludzie ją instruowali, ale dlatego, że miała własne potrzeby. Kaganowi i jego zespołowi wystarczyło to, by opisać ją jako rodzaj świadomości.

Byli bardzo ostrożni w sposobie, w jaki zdefiniowali to słowo: ""reaguje na wrażenia zmysłowe" poprzez adaptacyjne procesy wewnętrzne". Różni się to znacznie od szerzej rozumianego użycia tego terminu, który kojarzy się z subiektywnym doświadczeniem, a sam Kagan przyznaje, że DishBrain nie wykazał żadnych oznak prawdziwej świadomości.

Osobiście uważam, że jest to zbyt bezpieczne podejście. W 2016 roku neuroetolog Andrew Barron i filozof Colin Klein opublikowali artykuł, w którym argumentowali, że mózgi owadów wykonują podstawowe funkcje związane ze świadomością u ssaków. Pozyskują one informacje ze swojego środowiska, monitorują własne stany wewnętrzne i integrują te dane wejściowe w ujednolicony model, który generuje reakcje behawioralne. Wielu twierdzi, że subiektywne doświadczenie pojawia się w wyniku takiej integracji. Kręgowce, głowonogi i stawonogi są zbudowane tak, aby robić to na różne sposoby, więc ma to uzasadnienie, że mogą być fenomenalnie świadome. Można je nawet nazwać "istotami".

Weźmy na przykład Portia, rodzaj pająków, których improwizowane strategie łowieckie są tak wyrafinowane, że stworzenia te otrzymały przydomek "ośmionożnych kotów". Wykazują one dowody na wewnętrzną reprezentację, trwałość obiektów, przewidywanie i podstawowe umiejętności liczenia. Portia jest dzieckiem z plakatu dla argumentów Barrona i Kleina - ale ma tylko około 600 000 neuronów. DishBrain miał ich około 800 000. Jeśli Portia jest świadoma, dlaczego DishBrain - który uosabia wszystkie podstawowe warunki Barrona i Kleina - miałby nie być?

A DishBrain to dopiero pierwszy krok. Jego twórcy planują ulepszenie o 10 milionów neuronów (co dla każdego, kto interesuje się relatywizmem ewolucyjnym, jest skalą małych ryb/gadów) w sequelu. Inna grupa naukowców zaprezentowała organoid neuronowy, który nauczył się podstawowego rozpoznawania głosu. Warto zauważyć, że chociaż my, worki na mięso, dzielimy pewne miękkie pokrewieństwo z DishBrain, paradygmat wolnej energii ma zastosowanie do każdego złożonego, samoorganizującego się systemu. Jakakolwiek podstawowa świadomość porusza się w tym naczyniu, może równie łatwo zamanifestować się w krzemie. Możemy zaprogramować dowolne imperatywy w takich systemach, ale ich własne potrzeby będą nadal tykać pod spodem.

Trzeba przyznać, że diagram Venna obaw Geoffreya Hintona i ambicji Karla Fristona prawdopodobnie pokrywa się w miejscu, w którym przecinają się nauka i fikcja, gdzie świadoma sztuczna inteligencja - zdając sobie sprawę, że ludzkość jest zdecydowanie najbardziej chaotyczną i destabilizującą siłą na planecie - decyduje się nas unicestwić bez lepszego powodu niż uproszczenie świata z powrotem do pewnego przewidywalnego poziomu. Nawet ten scenariusz zawiera najcieńszą ze srebrnych podszewek: Jeśli minimalizacja darmowej energii jest poprawna, to świadoma maszyna z definicji ma niekompletny światopogląd. Popełnia błędy; jest ciągle pobudzana przez nieoczekiwane dane wejściowe i błędne przewidywania. Wciąż możemy ją zaskoczyć. Świadome maszyny mogą być inteligentne, ale przynajmniej nie są wszechwiedzące.

O wiele bardziej martwię się o to, co się stanie, gdy staną się na tyle inteligentne, by wrócić do snu.

https://web.archive.org/web/20240309153417/https://www.theatlantic.com/ideas/archive/2024/03/ai-consciousness-science-fiction/677659/
